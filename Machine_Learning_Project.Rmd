---
title: "Machine Learning Project"
author: "Wally Thornton"
date: "November 20, 2015"
output: html_document
---
##Executive Summary
Using data from a weight-lifting study, I seek a model to predict whether someone performing dumbbell curls performs them correctly. The dataset consists of 19,622 observations of 160 variables, although we'll see that not all these variables are needed to derive a sufficiently accurate model. I will explore the data, test a few models on the training set, tune the winning model, derive the out-of-sample error with validation and finally test the data to predict the form of 20 observations.
```{r setup, include=F}
knitr::opts_chunk$set(fig.width=7, fig.align='center')
options(scipen=999)
ensurePkg <- function(x) {
    if (!require(x,character.only = TRUE)) {
        install.packages(x,dep=TRUE, repos="http://cran.r-project.org")
        if(!require(x,character.only = TRUE)) stop("Package not found")
    }
}

ensurePkg("caret")
# ensurePkg("ggplot2")
ensurePkg("plyr")
ensurePkg("dplyr")
ensurePkg("corrplot")
ensurePkg("scales")
```

##Exploratory Data Analysis and Cleaning
First, I load the training and testing source files and then split the training dataset into validation (30%) and working (70%) sets, to better predict out-of-sample error rates. The validation set is not used for training the models at all, only for evaluating out-of-sample performance.
```{r splitData, message=F, warning=F}
train <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
set.seed(42)
trainI <- createDataPartition(train$classe, p = 0.70, list=F)
training <- train[trainI, ]
validation <- train[-trainI, ]
```

Examining only the working training set, I find that a number of factors consist of nothing but "#DIV/O" Excel errors, blanks and NAs, so these are excluded. Additionally, the summary variables (e.g., avg, stdev) and variables associated with the test (names, timeseries) are dropped. This leaves 71 potentially useful variables, so I also look for near-zero covariates and eliminate those, leaving 53 variables.

```{r exploreAndCleanData, message=F, warning=F}
workTrain <- training

# Remove summary variables
# avg*, stddev*, var*, min*, max* are all summary statistics and 100% NAs in the testing data
workTrain <- workTrain  %>% dplyr::select(-starts_with("avg"), 
                             -starts_with("stddev"),
                             -starts_with("var"), 
                             -starts_with("min"), 
                             -starts_with("max"),
                             -contains("timestamp"),
                             -contains("window"),
                             -X,
                             -user_name)

# Likewise, there are factors that have only "" and "#DIV/O" as factors:
workTrain <- workTrain %>% dplyr::select(-kurtosis_yaw_belt, 
                                         -skewness_yaw_belt, 
                                         -kurtosis_yaw_dumbbell, 
                                         -amplitude_yaw_belt, 
                                         -skewness_yaw_dumbbell, 
                                         -amplitude_yaw_dumbbell, 
                                         -kurtosis_yaw_forearm, 
                                         -skewness_yaw_forearm, 
                                         -amplitude_yaw_forearm)

#Remove variables with high proportion of missing data
propmiss <- function(dataframe) {
    m <- sapply(dataframe, function(x) {
        data.frame(
            nmiss=sum(is.na(x)), 
            n=length(x), 
            propmiss=sum(is.na(x))/length(x)
        )
    })
    d <- data.frame(t(m))
    d <- sapply(d, unlist)
    d <- as.data.frame(d)
    d$variable <- row.names(d)
    row.names(d) <- NULL
    d <- cbind(d[ncol(d)],d[-ncol(d)])
    return(d[order(d$propmiss), ])
}

missing <- propmiss(workTrain)
workTrain <- workTrain[,-as.integer(rownames(missing[missing$propmiss > 0.9,]))]

# Look for variables with just a few NAs and impute (none left so no changes made)
table(propmiss(workTrain)[2] > 0)

# Remove variables that have too few values (zero covariates)
nsv <- nearZeroVar(workTrain, saveMetrics = F)
workTrain <- workTrain[,-nsv]
```

##Modeling Choices
Comfortable for the moment with our 52 input and one output variables, I will test three classification models: a decision tree, a boosting model (AdaBoost) and random forest. 

###Decision Tree
After setting the seed and training a tree with k-fold cross validation (k of 10, repeated three times), the result is a tree with only 48.85% accuracy (note no 'D' is classified in any node). 

```{r basicTree, message=F, warning=F}
ensurePkg("rattle")
2
set.seed(42)
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
# trainControl() will default to k=10
fitTree <- train(classe ~ ., method = "rpart", data = workTrain, trControl = ctrl)
fancyRpartPlot(fitTree$finalModel)
predFitTree <- predict(fitTree, validation)
treeMatrix <- confusionMatrix(predFitTree, validation$classe)
```

###AdaBoost
I next trained an AdaBoost model using K-fold cross validation on the training set with k=5. Running the result against the validation set results in much better accuracy of 93%.
```{r adaBoost, message=F, warning=F}
ensurePkg("adabag")
ensurePkg("doParallel")
corecount <- detectCores()
cl <- makeCluster(corecount * 0.75)
registerDoParallel(cl)
getDoParWorkers()
set.seed(42)
fitAB <- boosting(classe ~ ., data = workTrain)
stopCluster(cl)

fitABPred <- predict(fitAB, validation)
fitABPred$confusion
fitABPred$error
```

###Random Forest
Before settling on AdaBoost as the winning model, I train and run a random forest, **again using k-fold cross validation, with k=10, repeated three times**. This results in the best off-the-bat accuracy of the three (99.2% on training set, 99.4% on validation set). It is run after setting up a cluster of CPU cores to enable parallel processing to reduce compute time.
```{r randomForest, message=F, warning=F}
corecount <- detectCores()
cl <- makeCluster(corecount * 0.75)
registerDoParallel(cl)
getDoParWorkers()
set.seed(42)
fitRF <- train(classe ~ ., importance = T, method = "rf", trControl = ctrl, data = workTrain) #mtry=2 is best
stopCluster(cl)
fitRF$finalModel
fitRF

predictFitRF <- predict(fitRF, validation)
rfMatrix <- confusionMatrix(predictFitRF, validation$classe)
```

##Model Selection
Plotting the confusion matrices, we can see visually what the accuracy and out-of-sample error rates tell us: random forest does a terrific job of predicting the `classe` output variable and will therefore be our model to tune and apply to the testing dataset.
```{r modelSelect, message=F, warning=F}
par(mfrow=c(1,3))
plot(treeMatrix[[2]], main="Confusion Matrix: Decision Tree")
plot(fitABPred$confusion, main="Confusion Matrix: AdaBoost")
plot(rfMatrix[[2]], main = "Confusion Matrix: Random Forest")
par(mfrow=c(1,1))
```

##Out of Sample Error
I used **k-fold cross-validation with k=10 and number of repeats equal to three when training the random forest**. Thanks to the cross-validation, that out-of-box error rate estimate of 0.72% is a good estimate of the model's out-of-sample error rate. Additionally, by running that cross-validated model on the validation set (which had **not** been used for training), we get an accurate **out-of-sample error estimate of `r percent(1 - rfMatrix$overall[[1]])`** (1 - highest accuracy):
```{r sampleError, warning=F, message=F}
rfMatrix$overall

## Pulls just accuracy and kappa
postResample(predictFitRF, validation$classe)
```

##Test
Finally, I run the trained and tuned random forest model on the test data, predicting the 'classe' for each of the 20 observations and submit for validation.
```{r testAndSubmit, message=F, warning=F}
# Run test
# Create character vector with 20 predictions, in order
answers <- predict(fitRF, testing)

# Then create and write files
pml_write_files <- function(x) {
    n = length(x)
    for(i in 1:n) {
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
    }
}
pml_write_files(answers)
```

The result is correct identification of all 20, supporting my modeling decisions and reinforcing that my low out-of-sample error rate was a valid estimate. 